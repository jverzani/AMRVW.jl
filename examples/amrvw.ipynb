{
  "cells": [
     {"cell_type":"markdown","source":"<h1>An overview of <code>AMRVW</code></h1>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The <code>AMRVW</code> package implements some numerical linear algebra algorithms of Jared L. Aurentz, Thomas Mach, Leonardo Robol, Raf Vandebril, and David S. Watkins for finding eigenvalues of matrices through Francis's method.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>An inspiration is to find  the roots of a polynomial through the eigenvalues of a companion matrix. This is implemented in <code>AMRVW</code> through the <code>roots</code> function.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>To illustrate,</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Complex{Float64},1}:\n 0.9999999999999996 + 0.0im\n 2.0000000000000027 + 0.0im\n 2.9999999999999876 + 0.0im\n  4.000000000000012 + 0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["using AMRVW\nconst A = AMRVW  # currently there are no exports\nps = [24.0, -50.0, 35.0, -10.0, 1.0]  #  (x-1)(x-2)(x-3)(x-4) = 24 -50x + 25x^2  -10x^3 +  x^4\nA.roots(ps)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The example shows</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul>\n<li><p>the interface expects polynomials specified through a vector of coefficients, <code>&#91;a0, a1, ..., an&#93;</code></p>\n</li>\n<li><p>the  4 roots, always as complex numbers</p>\n</li>\n<li><p>the fact that the roots are numeric approximations due to accumulated round off errors.</p>\n</li>\n</ul>","metadata":{}},
{"cell_type":"markdown","source":"<p>Similarly, roots of polynomials over the  complex numbers can be found</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5-element Array{Complex{Float64},1}:\n                   -1.0 + 2.220446049250313e-16im\n                   -0.0 + 1.0000000149011605im   \n                    0.0 + 0.9999999850988397im   \n 1.5265566588595902e-16 - 1.0im                  \n     1.0000000000000004 - 5.551115123125785e-17im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["ps  =  [0.0 + 1.0im, -1.0 + 0.0im, 0.0 + 0.0im, 0.0 + 0.0im, 0.0 - 1.0im, 1.0 + 0.0im] # (x-1)(x+1)(x-i)^2)(x+i)\nA.roots(ps)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>There are other ways  to numerically find roots  of polynomials  in <code>Julia</code>, notably the <code>roots</code> function  of  the <code>Polynomials</code> package and the <code>roots</code> function of the <code>PolynomialRoots</code> package:</p>","metadata":{}},
{"cell_type":"markdown","source":"<ul>\n<li><p>unlike <code>Polynomials.roots</code> (but similar to <code>PolynomialRoots.roots</code>) this  <code>roots</code> function can work with  big floats and other floating point types.</p>\n</li>\n<li><p>For moderate sized polynomials ($n \\approx 50$), <code>PolynomialRoots.roots</code> is faster than <code>Polynomials.roots</code> which is faster than <code>roots</code>, though all are fast. When $n \\approx 75$, <code>roots</code> is faster (much so for large $n$ than <code>Polynomials.roots</code>).</p>\n</li>\n<li><p>Unlike both <code>Polynomials.roots</code> and <code>PolynomialRoots.roots</code> this <code>roots</code> function can accurately identify roots of polynomials of high degree.  (For a polynomial with $n$ random coefficients, e.g., <code>ps &#61; rand&#40;n&#41;</code>) <code>Polynomials.roots</code> will have troubles for n around 50; <code>PolynomialRoots.roots</code> will have issues for <code>n</code> around 300; <code>roots</code> can quickly handle degree 3000, and still be accurate for higher degrees.)</p>\n</li>\n<li><p>The <code>roots</code> function is shown by  the authors  to be  backward stable. The same isn't the case  for the other  two.</p>\n</li>\n<li><p>In the first example, the residual errors are  similar in size to <code>Polynomials.roots</code>, but  <code>PolynomialRoots.roots</code> the residual errors seem to be generally a bit smaller.</p>\n</li>\n</ul>","metadata":{}},
{"cell_type":"markdown","source":"<h2>The companion matrix</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Both <code>roots</code> and <code>Polynomials.roots</code> use a companion matrix representation using the  eigenvalues of this matrix to identify the roots of the polynomial. (The  <code>PolynomialRoots.roots</code> function relies on a different method following a paper by <a href=\"https://arxiv.org/abs/1203.1034\">Skowron and Gould</a>.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Using some functions within <code>AMRVW</code> we can see the companion matrix:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4×4 Array{Float64,2}:\n  0.0   0.0   0.0  24.0\n -1.0   0.0   0.0  50.0\n  0.0  -1.0   0.0  35.0\n  0.0   0.0  -1.0  10.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["ps = [24.0, -50.0, 35.0, -10.0, 1.0]  #  (x-1)(x-2)(x-3)(x-4) = 24 -50x + 25x^2  -10x^3 +  x^4\nF = A.amrvw(ps)\nM = Matrix(F) |> round2   # round2 is just M -> round.(M, digits=2)"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Complex{Float64},1}:\n 0.9999999999999996 + 0.0im\n 2.0000000000000027 + 0.0im\n 2.9999999999999876 + 0.0im\n  4.000000000000012 + 0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["using LinearAlgebra\neigvals(F)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The eigvenvalues of this matrix indeed are the roots of the polynomials. Internally, <code>amrvw</code> actually uses an enlarged matrix, with an extra dimension that is not shown here.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Francis's Algorithm</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>Francis's Algorithm begins with a QR decomposition <code>M</code>. For example,</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearAlgebra.QRCompactWY{Float64,Array{Float64,2}}\nQ factor:\n4×4 LinearAlgebra.QRCompactWYQ{Float64,Array{Float64,2}}:\n 0.0  0.0  0.0  1.0\n 1.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0\n 0.0  0.0  1.0  0.0\nR factor:\n4×4 Array{Float64,2}:\n -1.0   0.0   0.0  50.0\n  0.0  -1.0   0.0  35.0\n  0.0   0.0  -1.0  10.0\n  0.0   0.0   0.0  24.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["LinearAlgebra.qr(M)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>the decomposition in <code>AMRVW</code> is slightly different, though similar</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4×4 Array{Float64,2}:\n  0.0   0.0   0.0  1.0\n -1.0   0.0   0.0  0.0\n  0.0  -1.0   0.0  0.0\n  0.0   0.0  -1.0  0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Matrix(F.QF)"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n 1.0  0.0  -0.0  -50.0   0.0\n 0.0  1.0  -0.0  -35.0   0.0\n 0.0  0.0   1.0  -10.0   0.0\n 0.0  0.0   0.0   24.0  -1.0\n 0.0  0.0   0.0    0.0   0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Matrix(F.RF) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>(Here the <code>RF</code> matrix shows the extra size used internally in the algorithm.)</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The idea of Francis's shifted algorithm is to identify shifts $\\rho_1$, $\\rho_2$, $\\dots$, $\\rho_m$ and generate a <em>unitary</em> matrix $V_0 = \\alpha (A-\\rho_1 I)(A-\\rho_2 I)\\cdots(A-\\rho_m)I \\cdot e_1$, $e_1$ being a unit vector with $1$ in the $1$ entry and $0$ elsewhere. As $V_0$ is unitary, the product $V_0^{-1}F V_0$ will have the same eigenvalues.  When $F$ is upper Hessenberg (upper triangular starting with the subdiagonal), as is the case with the companion matrix, then this product will be almost upper Hessenberg, save for a bulge.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>In the real-coefficient case,  $m=2$ is used to allow the calculations to be done over the real numbers. For the complex-coefficient case, $m=1$ is possible.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The following internal code demonstrates how to pull out a shift in the complex ($m=1$) case:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Complex{Float64},2}:\n 0.55+0.45im  -0.71+0.0im   0.0+0.0im  0.0+0.0im  0.0+0.0im\n 0.71+0.0im    0.55-0.45im  0.0+0.0im  0.0+0.0im  0.0+0.0im\n  0.0+0.0im     0.0+0.0im   1.0+0.0im  0.0+0.0im  0.0+0.0im\n  0.0+0.0im     0.0+0.0im   0.0+0.0im  1.0+0.0im  0.0+0.0im\n  0.0+0.0im     0.0+0.0im   0.0+0.0im  0.0+0.0im  1.0+0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["ps  =  [0.0 + 1.0im, -1.0 + 0.0im, 0.0 + 0.0im, 0.0 + 0.0im, 0.0 - 1.0im, 1.0 + 0.0im]\nF = A.amrvw(ps)\nM = Matrix(F); n = size(M, 1)\nMI = diagm(0 => ones(Complex{Float64}, 5)) # identity matrix\nstorage, ctr, m = A.make_storage(F), A.make_counter(F), 1\nA.create_bulge(F.QF, F.RF, storage, ctr) # finds shifts and creates V_0\n(V0 = storage.VU[1] * MI) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Up to rounding, $V_0$ is unitary:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["true"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["isapprox(V0 * V0', MI, atol=1e-8)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The matrix $V_0' M V_0$ has a bulge below the subdiagonal (the <code>&#91;3,1&#93;</code> position, illustrated with a <code>1</code> below):</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 BitArray{2}:\n 1  1  0  0  1\n 1  1  0  0  1\n 1  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  1"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["V0' * M * V0 |> round2 .|> !iszero"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The algorithm finds $V_1$ to chase the bulge downward:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Complex{Float64},2}:\n 1.0+0.0im   0.0+0.0im     0.0+0.0im   0.0+0.0im  0.0+0.0im\n 0.0+0.0im  0.11+0.57im  -0.82+0.0im   0.0+0.0im  0.0+0.0im\n 0.0+0.0im  0.82+0.0im    0.11-0.57im  0.0+0.0im  0.0+0.0im\n 0.0+0.0im   0.0+0.0im     0.0+0.0im   1.0+0.0im  0.0+0.0im\n 0.0+0.0im   0.0+0.0im     0.0+0.0im   0.0+0.0im  1.0+0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A.absorb_Ut(F.QF, F.RF, storage, ctr)\nA.passthrough_triu(F.QF, F.RF, storage, ctr, Val(:right))\nA.passthrough_Q(F.QF, F.RF, storage, ctr, Val(:right))\n(V1 = storage.VU[1] * MI) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Now this product will shift the  bulge downward to the <code>&#91;4,2&#93;</code> position:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 BitArray{2}:\n 1  1  1  0  1\n 1  1  1  0  1\n 0  1  1  0  1\n 0  1  1  0  0\n 0  0  0  1  1"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["V1' * (V0' * M * V0) * V1 |> round2 .|> !iszero"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>And again:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Complex{Float64},2}:\n  1.0+0.0im   0.0+0.0im    0.0+0.0im     0.0+0.0im    0.0+0.0im\n  0.0+0.0im   1.0+0.0im    0.0+0.0im     0.0+0.0im    0.0+0.0im\n -0.0+0.0im  -0.0+0.0im  -0.23+0.44im  -0.87+0.0im   -0.0+0.0im\n  0.0+0.0im   0.0+0.0im   0.87+0.0im   -0.23-0.44im   0.0+0.0im\n  0.0+0.0im   0.0+0.0im    0.0+0.0im     0.0+0.0im    1.0+0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A.passthrough_triu(F.QF, F.RF, storage, ctr, Val(:right))\nA.passthrough_Q(F.QF, F.RF, storage, ctr, Val(:right))\n(V2 = storage.VU[1] * MI) |> round2"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 BitArray{2}:\n 1  1  1  1  1\n 1  1  1  1  1\n 0  1  1  1  1\n 0  0  1  1  1\n 0  0  1  1  1"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["V2' * (V1' * (V0' * M * V0) * V1) * V2 |> round2 .|> !iszero"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Once pushed to the bottom, the bulge is absorbed into the matrix, leaving an upper Hessenberg form.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Shifts</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>If the shifts are appropriately chosen, after a few iterations this resulting matrix can be \"deflated\" so that he algorithm can work on a smaller matrix.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Above the bulge is created with a single rotator. As mentioned, for the real variable case, two rotators are used, so that the computations can be kept using real numbers. In general, the AMRVW algorithm can be defined for $m$ rotators. These rotators are produced by <code>create_bulge</code>, as illustrated above, and stored in <code>storage.UV</code>.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The choice of shifts is <em>essentially</em> the eigenvalues of the lower $2 \\times 2$ submatrix (In the $m$-shift case, the lower $m\\times m$ submatrix). In the Vandrebril and Watkins paper it is mentioned that this choice <em>normally</em> yields quaratic convergence. That is, one of the rotators will become a diagonal rotator with <code>s</code> part $0$. When that happens, deflation can occur. The algorithm is applied on this smaller, deflated matrix, until the deflated matrix is comprised of no more than $m$ rotators, at least one of which is a diagonal rotator. At this point one or more eigenvalues can be found. This quadratic convergence implies that generally there are $\\mathcal{O}(n)$ steps taken.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>The AMRVW decomposition of the companion matrix</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The main result of the two papers on \"Fast and Backward Stable Computation of Roots of Polynomials\" is a proof of backward stability of a method that utilizes a sparse factorization of both the <code>Q</code> and <code>R</code> parts of the QR decomposition of a companion matrix.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>Returning to the real case, and digging into some structures, we can illustrate:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["AMRVW.DescendingChain{Float64,Float64,Array{AMRVW.Rotator{Float64,Float64},1}}(AMRVW.Rotator{Float64,Float64}[AMRVW.Rotator{Float64,Float64}(0.0, 1.0, 1), AMRVW.Rotator{Float64,Float64}(0.0, 1.0, 2), AMRVW.Rotator{Float64,Float64}(0.0, 1.0, 3)])"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["ps = [24.0, -50.0, 35.0, -10.0, 1.0]\nF = A.amrvw(ps)\nF.QF.Q"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>To explain, this is a \"chain\" of real rotators, more clearly seen with:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["3-element Array{AMRVW.Rotator{Float64,Float64},1}:\n AMRVW.Rotator{Float64,Float64}(0.0, 1.0, 1)\n AMRVW.Rotator{Float64,Float64}(0.0, 1.0, 2)\n AMRVW.Rotator{Float64,Float64}(0.0, 1.0, 3)"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Vector(F.QF.Q)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>A rotator is a matrix which is identical to the identity matrix except in the <code>&#91;i,i&#43;1&#93; × &#91;i, i&#43;1&#93;</code> block, in which case it takes the form of a rotator: <code>&#91;c s; -s c&#93;</code>. (Our rotators are in the different direction than those in the papers.) Here <code>c</code> and <code>s</code> are the cosine and sine of some angle. These rotators are indexed by <code>i</code> and we use the notation $U_i$ to indicate a rotator of this form for a given $i$. In the above, we  can see  with inspection that there are 3 rotators with $i$ being 1, 2, and 3. This set of rotators is \"descending\" due to their order (1 then 2 then 3); ascending would be 3 then 2 then 1. The product of descending rotators will be upper Hessenberg:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4×4 Array{Float64,2}:\n  0.0   0.0   0.0  1.0\n -1.0   0.0   0.0  0.0\n  0.0  -1.0   0.0  0.0\n  0.0   0.0  -1.0  0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Matrix(F.QF.Q)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>A rotator at level $i$ will commute with a rotator at level $j$ unless $|i-j| \\leq 1$. In the case where $i-j = \\pm 1$, a key computation is the \"turnover\", which represents $U_i V_j W_i$ as $VV_j WW_i UU_j$. With the turnover, we can easily pass a rotator through an ascending or descending chain without disturbing those patterns.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>In the  above illustration of Francis's  algorithm, the matrices  $V_0$, $V_1$,  etc. can be seen to be  rotators of this type. More generally, a unitary matrix with $m$ shifts can be viewed as a product of $m$  such rotators.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The $R$ decomposition  is trickier.  In the initial QR decomposition, $R$ has a simple structure plus a rank one part (coming from the coefficients). The  decomposition has two chains, an ascending one, <code>Ct</code>, and a descending one, <code>B</code>:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["AMRVW.DescendingChain{Float64,Float64,Array{AMRVW.Rotator{Float64,Float64},1}}(AMRVW.Rotator{Float64,Float64}[AMRVW.Rotator{Float64,Float64}(-0.7536071065605803, 0.6573251318346122, 1), AMRVW.Rotator{Float64,Float64}(-0.8025327939615967, 0.5966080075025758, 2), AMRVW.Rotator{Float64,Float64}(-0.3843312210120439, 0.9231952732523013, 3), AMRVW.Rotator{Float64,Float64}(-0.04163054471218133, -0.999133073092352, 4)])"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Ct = F.RF.Ct\nB = F.RF.B"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>These almost begin as inverses:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n  1.0   0.0   0.0  0.0   0.0\n -0.0   1.0   0.0  0.0   0.0\n -0.0  -0.0   1.0  0.0   0.0\n  0.0   0.0  -0.0  0.0  -1.0\n  0.0   0.0   0.0  1.0   0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["MI = diagm(0 => ones(5))\n(Z = Ct * (B * MI)) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>However, <code>Ct</code> is cleverly chosen to encode the rank 1 part. This can be uncovered through the following:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.015072142131211606"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["e1 = vcat(1, zeros(4))\nen1 = vcat(zeros(4), 1)\nrho = (en1' * (Ct * MI) * e1)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>and</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n  0.0   0.0  -0.0  -50.0   0.0\n  0.0   0.0  -0.0  -35.0   0.0\n  0.0   0.0  -0.0  -10.0   0.0\n  0.0   0.0   0.0   24.0   0.0\n -0.0  -0.0  -0.0   -1.0  -0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["yt = -(1/rho * en1' * (Ct*(B*MI)))\nCt * (e1 * yt) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Leading to <code>R</code>:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n 1.0  0.0  -0.0  -50.0   0.0\n 0.0  1.0  -0.0  -35.0   0.0\n 0.0  0.0   1.0  -10.0   0.0\n 0.0  0.0   0.0   24.0  -1.0\n 0.0  0.0   0.0    0.0   0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Z + Ct * (e1 * yt) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The algorithm passes a rotator through this decomposition, which in turn relies on passing a rotator through the two chains <code>B</code> and <code>Ct</code>, which, with the turnover computation, is easily computed.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>With these decompositions in mind, the computation above $V_0' M V_0$, can be seen as $U_1' Q R U_1$.  The product $U_1' Q = U_1' Q_1 Q_2 \\cdots Q_k$ is just a product of two rotators at level 1, so $U_1' Q_1$ can be fused to give a new $\\tilde{Q}_1$ in the descending chain factorization of $Q$.  The passthrough just mentioned allows $\\tilde{Q} R U_1$ to have this form $\\tilde{Q} \\tilde{U}_1 \\tilde{R}$ and by passing through the descending chain, we have this form $U_2 \\hat{Q} \\tilde{R}$. The matrix $V_1$ (of Francis's algorithm above) is seen to be $U_2$, as the similarity transform using $Q_1=U_2$ leaves the product $\\hat{Q} \\tilde{R} U_2$ having the same eigen values, but with the bulge shifted down one level. This basic idea forms the algorithm to chase the bulge. In the $m > 1$ case, some other details are included.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The decomposition of the companion matrix is sparse. Rather than require $O(n^2)$ storage, it only needs $O(n)$. The iterative algorithm is $O(n)$ per iteration  and  $O(n^2)$ overall, as compared to the $O(n^3)$ required in general. This reduction allows the method to be practical for large $n$, unlike <code>Polynomial.roots</code> which uses an $O(n^3)$ algorithm.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h3>Pencil decompositions</h3>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>In \"<em>Fast and backward stable computation of roots of polynomials, Part II</em>\" the method is extended to the pencil decomposition of a polynomial.  A pencil decomposition of a polynomial, is a specification where if $p = a_0 + a_1x^1 + \\cdots + a_n x^n$ then $v_1 = a_0$, $v_{i+1} + w_i = a_i$, and $w_n = a_n$. This has some advantages in cases where the polynomial has a particularly small leading coefficient, since division by a tiny $a_n$ will result in very large entries. The algorithm uses two upper triangular matrices.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The <code>roots</code> function allows a pencil decomposition to be passed in as two vectors:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{Complex{Float64},1}:\n 0.9999999999999978 + 0.0im\n  2.000000000000016 + 0.0im\n  2.999999999999957 + 0.0im\n  4.000000000000038 + 0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["ps = [24.0, -50.0, 35.0, -10.0, 1.0]\nvs, ws = A.basic_pencil(ps)\nA.roots(vs, ws)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<h4>The Wilkinson polynomial</h4>","metadata":{"internals":{"slide_type":"subslide"},"slideshow":{"slide_type":"subslide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The Wilkinson polynomial, $(x-1)(x-2)\\cdots(x-20)$, poses a challenge for <code>roots</code>:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["20-element Array{Complex{Float64},1}:\n 0.9999999999999314 + 0.0im                 \n 2.0000000000690283 + 0.0im                 \n  2.999999984047963 + 0.0im                 \n   4.00000097471277 + 0.0im                 \n  4.999975992180495 + 0.0im                 \n  6.000307520200189 + 0.0im                 \n  6.997597867096526 + 0.0im                 \n  8.013488320937116 + 0.0im                 \n  8.947674648356376 + 0.0im                 \n 10.392884549364062 - 0.046155844297629416im\n 10.392884549364062 + 0.046155844297629416im\n 12.333430486608142 - 0.7766219381354028im  \n 12.333430486608142 + 0.7766219381354028im  \n  14.47118315549315 - 1.0642088782328376im  \n  14.47118315549315 + 1.0642088782328376im  \n  16.67497994473274 - 0.8714271268945672im  \n  16.67497994473274 + 0.8714271268945672im  \n 18.540903438336155 + 0.0im                 \n 18.740138141986936 + 0.0im                 \n 20.014956839679538 + 0.0im                 "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["import Polynomials\nps = Polynomials.coeffs(Polynomials.poly(1.0:20.0))\nA.roots(ps)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The answer involves complex-valued roots, even though the roots are clearly integer valued. (The <code>Polynomials.roots</code> function and <code>PolynomialRoots.roots</code> do get  real answers for  this polynomial). As an aside, the exact implementation of the fundamental <code>turnover</code> operation will effect the number of such roots.  The issue of spurious complex-valued roots might be addressed by separating out the smaller coefficients from the large ones. Here is a function to split a polynomial into two pieces.</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["pencil_split (generic function with 1 method)"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["function pencil_split(ps, n)\n    vs = zeros(eltype(ps), length(ps)-1)\n    ws = zeros(eltype(ps), length(ps)-1)\n\n    vs[1:n] = ps[1:n]\n    ws[n:end] = ps[n+1:end]\n\n    vs, ws\nend"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>It turns out that with <code>n&#61;13</code> only real roots are identified:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["20-element Array{Complex{Float64},1}:\n 0.9999999999999111 + 0.0im\n  2.000000000045409 + 0.0im\n  2.999999993790896 + 0.0im\n  4.000000299546987 + 0.0im\n 4.9999928688726465 + 0.0im\n  6.000097015047115 + 0.0im\n  6.999196038446284 + 0.0im\n  8.004256321970168 + 0.0im\n  8.985938176786837 + 0.0im\n 10.030592997286739 + 0.0im\n 10.964284539260134 + 0.0im\n 12.010231202120066 + 0.0im\n 13.020869369857817 + 0.0im\n 14.009268935344197 + 0.0im\n 14.898793643212057 + 0.0im\n 16.215709156415652 + 0.0im\n 16.798926258829525 + 0.0im\n 18.087132356819428 + 0.0im\n 18.971255873673368 + 0.0im\n 20.003454952675025 + 0.0im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["A.roots(pencil_split(ps, 13)...)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The pencil here does a better job with this polynomial, but the choice of <code>13</code> was made with hindsight, not foresight.</p>","metadata":{}},
{"cell_type":"markdown","source":"<h2>Other uses</h2>","metadata":{"internals":{"slide_type":"subslide","slide_helper":"subslide_end"},"slideshow":{"slide_type":"slide"},"slide_helper":"slide_end"}},
{"cell_type":"markdown","source":"<p>The <code>AMRVW.jl</code> package allows some other applications, though the exact interface needs to be settled on.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>If we take rotators $Q_1, Q_2, \\dots, Q_k$ their product will be upper Hessenberg:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{AMRVW.Rotator{Float64,Float64},1}:\n AMRVW.Rotator{Float64,Float64}(0.9898849414884573, 0.1418724871650363, 1) \n AMRVW.Rotator{Float64,Float64}(0.8341387938282723, 0.5515545962375034, 2) \n AMRVW.Rotator{Float64,Float64}(0.1514032162650409, 0.9884720866593054, 3) \n AMRVW.Rotator{Float64,Float64}(0.9951811955878973, 0.09805298541219026, 4)"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Qs = A.random_rotator.(Float64, [1,2,3,4])"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n  0.99   0.12   0.01   0.08  0.01\n -0.14   0.83   0.08   0.54  0.05\n  0.0   -0.55   0.13   0.82  0.08\n  0.0    0.0   -0.99   0.15  0.01\n  0.0    0.0    0.0   -0.1   1.0 "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["MI = diagm(0 => ones(5))\n(M = Qs * MI) |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Their eigenvalues can be found:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5-element Array{Complex{Float64},1}:\n 0.05268872143733358 - 0.998610984634807im  \n 0.05268872143733358 + 0.998610984634807im  \n  0.9911775276495882 - 0.13254096982612096im\n  0.9911775276495882 + 0.13254096982612096im\n                 1.0 + 0.0im                "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["eigvals(M)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>But the sparse representation can be used to also find such eigenvalues:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n  0.99   0.12   0.01   0.08  0.01\n -0.14   0.83   0.08   0.54  0.05\n  0.0   -0.55   0.13   0.82  0.08\n  0.0    0.0   -0.99   0.15  0.01\n  0.0    0.0    0.0   -0.1   1.0 "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["QF = A.q_factorization(A.DescendingChain(Qs))\nF = A.QRFactorization(QF)  # defaults to identify R factorization\nMatrix(F) |> round2 # same as M"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5-element Array{Complex{Float64},1}:\n 0.05268872143733375 - 0.9986109846348071im \n 0.05268872143733375 + 0.9986109846348071im \n  0.9911775276495888 - 0.13254096982612096im\n  0.9911775276495888 + 0.13254096982612096im\n                 1.0 + 0.0im                "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["eigvals(F)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The <code>qr_factorization</code> method can take a Hessenberg matrix and complete the factorization. For this case, we have:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5-element Array{Complex{Float64},1}:\n 0.052688721437333615 - 0.9986109846348071im \n 0.052688721437333615 + 0.9986109846348071im \n   0.9911775276495887 - 0.13254096982612096im\n   0.9911775276495887 + 0.13254096982612096im\n                  1.0 + 0.0im                "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["F = A.qr_factorization(M, unitary=true)\neigvals(F)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>When <code>unitary&#61;true</code> is the case, this will outperform <code>eigvals</code> once the matrix size is around 50 by 50 and is significantly more performant for the 150 by 150 case.</p>","metadata":{}},
{"cell_type":"markdown","source":"<hr />","metadata":{}},
{"cell_type":"markdown","source":"<p>Not all upper Hessenberg matrices can he expressed as a descending chain of rotators, as the latter is unitary. However, any upper Hessenberg matrix can easily be seen to be represented as a descending chain of rotators times an upper triangular matrix.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>The Givens rotation is a rotator, $U$, based on a $c$ and $s$, chosen so that if $x= [a,b]$, then $[c s;-s conj(c)] x = [r,0]$. This allows, for example, the following:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n 0.191261  0.819292  0.246846  0.110906  0.956429\n 0.321516  0.406893  0.335929  0.280307  0.31258 \n 0.0       0.122515  0.1062    0.767804  0.650721\n 0.0       0.0       0.154222  0.336698  0.967559\n 0.0       0.0       0.0       0.307407  0.639899"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["M = triu(rand(5,5), -1)  # upper Hessenberg"],"metadata":{},"execution_count":1},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n 0.37   0.77   0.41  0.3    0.76\n 0.0   -0.5   -0.04  0.05  -0.66\n 0.0    0.12   0.11  0.77   0.65\n 0.0    0.0    0.15  0.34   0.97\n 0.0    0.0    0.0   0.31   0.64"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["c,s,r = A.givensrot(M[1,1], M[2,1])\nU1 = A.Rotator(c,s,1)\nU1 * M |> round2"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>That is the subdiagonal in column 1 is 0 Similarly, a <code>U2</code> could then be found so that subdiagonal in column 2 is 0, etc. That is a choice of rotators is available for $U_k U_{k-1} U_{k-2} \\cdots U_2 U_1 M = R$. Setting $V_i = U_i'$, we have then $M = V_1 V_2 \\cdots V_k R = QR$, where $Q$ is unitary and  in decomposed form, and $R$ is upper triangular.</p>","metadata":{}},
{"cell_type":"markdown","source":"<p>For example:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["4-element Array{AMRVW.Rotator{Float64,Float64},1}:\n AMRVW.Rotator{Float64,Float64}(0.5112526559744986, -0.8594304635972714, 1)  \n AMRVW.Rotator{Float64,Float64}(-0.9708338787898337, -0.23975316430422075, 2)\n AMRVW.Rotator{Float64,Float64}(-0.5180915253123047, -0.855325184593304, 3)  \n AMRVW.Rotator{Float64,Float64}(0.8384618673746593, -0.5449602709910136, 4)  "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["Us = Any[]\nG = copy(M)\nfor i in 1:4\n  c,s,r = A.givensrot(G[i,i], G[i+1,i])\n  Ui =  A.Rotator(c,s,i)\n  pushfirst!(Us, Ui)\n  lmul!(Ui, G)\nend\nR = G\nQs = reverse(adjoint.(Us))"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>With this, we can do the following:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×5 Array{Float64,2}:\n 0.0   0.0  -0.0  -0.0   0.0\n 0.0  -0.0  -0.0   0.0  -0.0\n 0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0  -0.0   0.0\n 0.0   0.0  -0.0  -0.0  -0.0"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["QF = A.q_factorization(A.DescendingChain(Qs))\nRF = A.RFactorizationUpperTriangular(R)\n\nF = A.QRFactorization(QF, RF)\n\nMatrix(F)  - M |> round2# same as F"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>And</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×2 Array{Complex{Float64},2}:\n -0.247892-0.0325955im  -0.247892-0.0325955im\n -0.247892+0.0325955im  -0.247892+0.0325955im\n  0.190355+0.0im         0.190355+0.0im      \n   0.83952+0.0im          0.83952+0.0im      \n   1.14686+0.0im          1.14686+0.0im      "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["[eigvals(F) eigvals(M)]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>This patttern is encoded in the <code>qr_factorization</code> function, mentioned above. For any Hessenberg matrix it can be employed:</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5-element Array{Complex{Float64},1}:\n  -0.4452548492383729 + 0.0im                \n -0.14556669030117644 + 0.0im                \n  0.23143930085959272 - 0.10378641933434699im\n  0.23143930085959272 + 0.10378641933434699im\n   2.2535768640017784 + 0.0im                "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["H = hessenberg(rand(5,5))\nF = A.qr_factorization(H.H)\neigvals(F)"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>This approach is in the ballpark with <code>eigvals&#40;Matrix&#40;H.H&#41;&#41;</code>, though slower by a factor performance-wise, as the <code>R</code> part is not factored into rotators, so uses $\\mathcal{O}(n)$ operations – not $\\mathcal{O}(1)$ – in each step of the algorithm. (When <code>unitary&#61;true</code> is the case, the <code>R</code> part is much faster, so is competitive.</p>","metadata":{}},
{"cell_type":"markdown","source":"<hr />","metadata":{}},
{"cell_type":"markdown","source":"<p>The product of a descending chain of rotators is upper Hessenberg and the product of an  ascending chain or rotators is lower Hessenberg. With modifications, described in \"A generalization of the multishift QR algorithm\" a related algorithm can be employed. A \"twisted chain\" is one where a descending chain of rotators is permuted, an ascending chain being one possible twisting among many others.</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["5×2 Array{Complex{Float64},2}:\n  0.32193-0.946763im   0.32193-0.946763im\n  0.32193+0.946763im   0.32193+0.946763im\n 0.943362-0.331765im  0.943362-0.331765im\n 0.943362+0.331765im  0.943362+0.331765im\n      1.0+0.0im            1.0+0.0im     "]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["N = 4\nT = Float64; S = T # real case here\nMI = diagm(0 => ones(N+1))\nQs = A.random_rotator.(T, [4,3,2,1])\nM = Qs * MI\n\nQF = A.q_factorization(A.TwistedChain(Qs))\nF = A.QRFactorization(QF)   # use default identify R factorization\n\n[eigvals(M) eigvals(F)]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>Here is another example with a CMV pattern to the twisted</p>","metadata":{}},
{"outputs":[{"output_type":"execute_result","data":{"text/plain":["6×2 Array{Complex{Float64},2}:\n -0.215272-0.976554im   -0.215272-0.976554im \n -0.215272+0.976554im   -0.215272+0.976554im \n  0.758803-0.65132im     0.758803-0.65132im  \n  0.758803+0.65132im     0.758803+0.65132im  \n  0.995491-0.0948554im   0.995491-0.0948554im\n  0.995491+0.0948554im   0.995491+0.0948554im"]},"metadata":{},"execution_count":1}],"cell_type":"code","source":["N = 5\nMI = diagm(0 => ones(N+1))\nQs = A.random_rotator.(T, [1,3,5,2,4])\nM = Qs * MI\n\nQF = A.q_factorization(A.TwistedChain(Qs))\n\nF = A.QRFactorization(QF)\n\n[eigvals(M) eigvals(F)]"],"metadata":{},"execution_count":1},
{"cell_type":"markdown","source":"<p>The implementation for twisted chains is not nearly as efficient as that for descending chains.</p>","metadata":{}}
    ],
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  },
 "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  }

 },
 "nbformat": 4,
 "nbformat_minor": 2

}
